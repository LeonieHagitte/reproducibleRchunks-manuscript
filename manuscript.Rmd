---
title             : "Automated Reproducibility Testing in R Markdown"
shorttitle        : "Reproducibility Testing"
author: 
  - name          : "Andreas M. Brandmaier"
    affiliation   : "1,2,3"
    corresponding : no    # Define only one corresponding author
    address       : "Rüdesheimer Str. 50, 14197 Berlin"
    email         : "andreas.brandmaier@medicalschool-berlin.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Methodology"
      - "Software"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

  - name          : "Aaron Peikert"
    affiliation   : "2,3"
    corresponding : no    # Define only one corresponding author
    email         : "peikert@mpib-berlin.mpg.de"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Writing - Review & Editing"
      - "Methodology"
            
affiliation:
  - id            : "1"
    institution   : "Department of Psychology, MSB Medical School Berlin"
  - id            : "2"
    institution   : "Center for Lifespan Psychology, Max Planck Institute for Human Development"
  - id            : "3"
    institution   : "Max Planck UCL Centre for Computational Psychiatry and Ageing Research"

#note: Preprint submitted for publication

authornote: |
  Please address correspondence to: Andreas M. Brandmaier, Lentzeallee 94, 14195 Berlin, Germany. Email: brandmaier@mpib-berlin.mpg.de.
  The R package described in this article can be downloaded from https://github.com/brandmaier/reproducibleRchunks. The manuscript source code is available at https://github.com/brandmaier/reproducibleRchunks-manuscript. We thank Leonie Hagitte for providing comments on an earlier version of the manuscript.

abstract: |
 Computational results are considered _reproducible_ if the same computation on the same data yields the same results if performed on a different computer or on the same computer later in time. Reproducibility is a prerequisite for replicable, robust and transparent research in digital environments.
 Various approaches have been suggested to increase chances of reproducibility of R code.
 Many of them rely on R Markdown as a language to dynamically generate reproducible research assets (e.g., reports, posters, or presentations).
 However, a simple way to automatically verify reproducibility is still missing.
 We introduce the R package `reproducibleRchunks`, which automatically stores metadata about original computational results and verifies later reproduction attempts automatically.
 With a minimal change to users' workflows, we hope that this approach increases transparency and trustworthiness of digital research assets.
  
keywords          : "reproducibility, R, Markdown, Open Science, statistical analysis, computation"
wordcount         : "`r tryCatch(wordcountaddin::word_count(here::here('manuscript.Rmd')))`"

bibliography      : "r-references.bib"

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man" #man
output            : papaja::apa6_pdf
header-includes:
   - \usepackage{amsmath}  # providing non-breaking mbox environment
---

```{r setup, include = FALSE}
library("papaja")
library("reproducibleRchunks")
library("knitr")
r_refs("r-references.bib")
#r_refs("r-references-anonymized.bib")
```

```{r}
# Initialize an empty list to store legends
legends <- list()

# get hook
original_plot_hook = knitr::knit_hooks$get('plot')

# Define a hook to capture the figure legends
knitr::knit_hooks$set(plot = function(x, options) {
  if (!is.null(options$fig.cap)) {
    legends <<- c(legends, options$fig.cap)
  }
  # Return the original plot hook
  original_plot_hook(x, options)
})
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed,
                      out.width = "80%")
```

Computational results, including those from statistical data analyses, are considered _reproducible_ if the same computation on the same data yields the same results if performed on a different computer or on the same computer later in time. Reproducibility is a prerequisite for replicable, robust and credible research in digital environments [@epskamp2019reproducibility; @hardwicke2018data].
Surprisingly often, results from published raw data cannot be reproduced [@artner2021reproducibility].
 Various approaches have been suggested to increase chances of reproducibility of statistical data analyses in the R language [@peikert2021reproducible; @renv2022; @chan2023rang; @nagraj2023pracpac].
 Many approaches rely on R Markdown [@rmarkdown2023].
R Markdown is a simple language, which allows mixing natural language, simple formatting instructions (e.g., what is a headline, what should appear in bold face, what are list elements) and computer code (R, Python or others) in a single document [@xie2018r].
Whenever an R Markdown document is reproduced, all of its computer code chunks are executed, which allows for the generation of dynamic content in the document.
This is particularly interesting for quantitative research reports, as  statistical results and contents of tables and figures can be dynamically generated.
Packages like `papaja` [@R-papaja] or `stargazer` [@R-stargazer] further extend R Markdown's capabilities to format documents (including tables, figures, and statistical results) in a standardized style, such as APA style.
As one example, (R) Markdown forms the basis of _executable research articles_ [@tsang2020welcome], a browser-based form of research articles that allows authors to enrich their publications with interactive elements, such as figures or results that dynamically change based on readers' input.
A particular advantage of using R Markdown for creating research reports is that it helps avoiding common threats to reproducibility of statistical results, first and foremost, copy&paste errors, which happen when results of a regular R script and a scientific report (e.g., a reported statistic or table in a Word document) mismatch by mistake [@peikert2021reproducible];
further, R Markdown is suitable to generate a variety of scientific assets other than manuscripts such as posters [@posterdown2019], presentations [@xaringan2022], resumés [@vitae2023] or preregistrations [@peikert2021reproducible] from the same Markdown language.

When computations are repeated later in time with the goal of reproducing results computed earlier, it is hard and laborious to check whether results are identical or not. 
A typical case, in which verification of results is relevant, would be that readers or reviewers download open data and open code accompanying a manuscript and attempt to re-run the code on their computer. 
In order to test whether these open materials reproduced, they would have to manually compare the newly computed results with the published manuscript -- and, often, it would not be immediately clear which results mapped to what parts of the manuscript. 
And even with dynamically-generated R Markdown documents, there is no straightforward way of checking reproducibility in an automated way.

The ´reproducibleRchunks` package addresses the challenge of ensuring computational results in R Markdown documents are automatically testable for successful reproduction. 
This means verifying whether the same script with the same data yields identical results, even on different computers or at later times, with minimal disruption to users' workflows.
This article is intended for readers who use R for statistical analyses and ideally already had some experience with R Markdown. 
We aim to first offer a less technical tutorial outlining a typical use case for the package. 
Following this, we provide more detailed technical descriptions of the package's mechanics and various ways to customize its behavior.
We believe that the package can be used effectively without a deep understanding of the technical details. 
However, gaining insight into these details may enhance the user experience and help readers understand the challenges involved in ensuring reproducibility.


For users already familiar with R Markdown, only two modifications are needed when using this package: 
1. Load the reproducibleRchunk package in the first code chunk of your document
2. Change the code chunk type from `r` to `reproducibleR` for each code chunk that requires automatic verification in the future.
In `reproducibleR` chunks, all newly declared variables are automatically identified and their contents are stored in a metadata file. 
Furthermore, information about the code chunk itself (that is, the code syntax) is also stored, such that later non-reproduction can be traced back to either changes of the code chunk or changes of the computation.
Once a document is reproduced, all computational results generated in the `reproducibleR` chunks are automatically tested for reproducibility.
Successes and failures of the reproduction attempts are displayed in a reproducibility report for each chunk.
Customizations or suppression of these reports are possible as we will demonstrate later.
A straightforward way to assess reproducibility of computational results would be a bitwise comparison of the entire knitted^[This is R Markdown language for generating a publishable document from a Markdown source file] Markdown document (i.e., the resulting HTML, PDF or DOCX file) to a previous version.
Yet, a particular advantage of using `reproducibleR` chunks is that users have a fine-grained control over the stages of a data analysis or any other computation that should be checked for reproducibility. 
This allows for forensic examination of non-reproducibility; for example, finding out whether the non-reproducibility is due to changes in the data file, due to the data cleaning or the estimation of a statistical model. 
Using `reproducibleRchunks`, users can separately assess the reproducibility of each stage of a given statistical data analysis (or any other computation).
Also, not all elements of a research asset may be required to reproduce (such as the current date displayed in a presentation of results).
In the following, we will briefly demonstrate how reproduction will be checked and how successful and failed attempts are displayed by the package.
We will go into details of the mechanics of the package, and discuss potential cases of non-reproducibility and how the package will support researchers in handling these cases.

# Introductory Example

The core functionality of the package is to provide  `reproducibleR` chunks, which are code chunks in R Markdown documents that can be used like regular `R`-chunks (including name labels and the usual options regulating output parameters) but offer reproducibility checks in addition. 
To use these new code chunks, the package must be loaded^[If an R Markdown file is knitted and the `reproducibleRchunks` package was not loaded, this error is shown: Error `In get_engine(options$engine) :  Unknown language engine 'reproducibleR'`] in the first regular `R` code chunk in the document using

```{r eval=FALSE, echo=TRUE}
library(reproducibleRchunks)
```

On load, the package registers a new type of code chunk called `reproducibleR`.
Figure \@ref(fig:rstudioscreen1) shows a snippet from an R Markdown file, in which there is a `reproducibleR` chunk with the name _addition_. 
In the chunk, a new variable `my_sum` is declared and defined to be the sum of some variable `x` plus one. 
Variable `x` was declared in an earlier code chunk (not shown in the figure) and is thus not subject to the reproducibility test of this chunk. 

```{r rstudioscreen1, fig.cap="Automated testing of variables for reproducibility is initiated by using code chunks of type `reproducibleR` (see yellow highlight) instead of `R` as shown in this example R Markdown snippet. "}
knitr::include_graphics("img/rstudio-screenshot-marker2.png")
```

```{r rstudioscreen2, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is generated for the first time."}
knitr::include_graphics("img/generation-step1.png")
```

In this example, we assume that the R Markdown document is eventually converted into a HTML-report (but all other pandoc-supported formats are supported as well, such as PDF or Word documents).
During initial document conversion, the R code in the `reproducibleR`-chunk is executed and the value of `my_sum` is stored in a separate file (with prefix _.repro_), which contains all information for further reproducibility verification tests. 
A reproducibility report is shown in the generated document below the code chunk indicating information about all variables of the given chunk. Figure \@ref(fig:rstudioscreen2) show a snippet from the HTML report generated during the initial creation of the document.

Once the same document is reproduced, that is generated a second time, and the reproducibility metadata file exists, the computations in the code chunk are rerun, and their results are compared to the stored reproducibility information for each variable.
The reproducibility report will then detail the success of each reproduction attempt.
In our example, the computation was successfully reproduced.
A snippet from the resulting generated document is displayed in Figure \@ref(fig:rstudioscreen3).

```{r rstudioscreen3, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is re-generated and a variable is successfully reproduced."}
knitr::include_graphics("img/generation-step2.png")

```

If the document is reproducedj and the computational result differs from the original result, this failure to reproduce is noted in the reproducibility report.
In a variation of the previous example, we assume someone changed the value of `x`, then variable `my_sum` would change and the a failure would be displayed (see Figure \@ref(fig:rstudioscreen4)).

```{r rstudioscreen4, fig.cap="Snippet from a knitted Markdown file showing the reproducibility report when a document is re-generated and the reproduction of a variable failed.", fig.pos="b"}
knitr::include_graphics("img/generation-step2-failed.png")

```

Ideally, the reproducibility metadata files are always kept together with the original Markdown file. For example, when providing Open Code on open platforms, the repository should contain both the Markdown files as well as the metadata files. Note that the metadata file format is particularly suitable to be used with version control systems [see @peikert2021reproducible].

# Methods

In the following, we describe how the package stores, retrieves and compares reproducibility information to verify reproduction attempts.
First of all, the package executes `reproducibleR` code chunks just like regular R code chunks, meaning there are no additional restrictions on what can be computed and tested for reproducibility.
After code execution, the package collects information about all variables that were newly declared in the current chunk. 
The contents of those variables are stored in a separate JSON data file.
JSON, short for JavaScript Object Notation, is an open standard for mapping complex objects to text files with high simplicity and readability for machines and humans [@lennon2009introduction].
The name of the JSON file contains the original Markdown file and the chunk label and, by package default, starts with the prefix `.repro`. 
That is, reproducibility information of a chunk labelled `datacleaning` in the file `sem_analysis.Rmd` is stored in a file called _.repro_sem_analysis.Rmd_datacleaning.json_.
Once the document is re-generated and matching JSON data files exist, their content is checked against the newly computed chunk variables for identity.

```{r echo=FALSE, eval=TRUE}
set.seed(42)
numbers <- sample(1:10, 5)
```

Here is an example of how the contents of a single variable called `numbers` is stored in JSON format. Here, the variable content is a vector of five random draws from the number one to ten: `r numbers` generated by the following command:

```{r echo=TRUE, eval=FALSE}
set.seed(42)
numbers <- sample(1:10, 5)
```

This vector can be serialized in raw format to a JSON format as follows:


\begin{verbatim}
```{r echo=FALSE, eval=TRUE, results="asis"}
content <- jsonlite::serializeJSON(numbers, pretty=TRUE)
content <- gsub(pattern="\\n",replacement="  \n", x=content)
cat(content)
```
\end{verbatim}

The JSON format has a clear, formal structure, which allows easy parsing of the information both for computers and humans. 
Without going into detail, for each object, its type (e.g., character, integer, list, or class), its value and its attributes are stored. The stored values themselves values  can in turn consist of collections of objects (e.g., lists of lists); in this way, arbitrarily complex R objects (such as an entire regression model) can be represented.
For privacy reasons and to save disk space, we actually do not store the raw data by default.
Instead, we store "fingerprints" of the data.
Digital fingerprints are fixed-length representations of an arbitrarily large digital object.
They are generated such that small changes in the input object will lead to different fingerprints and, thus, they are ideally suited to verify integrity of the original data.
The idea is very similar to a check sum digit in a credit card number, which can be computed from the remaining digits to check whether the credit card number is valid.
Fingerprints are realized via one-way hash functions, which can take an arbitrary large digital object and map it onto a fixed-size object, which is typically displayed as hexadecimal string.
As default, we chose the SHA256 algorithm, which allows us to map any content of a variable in R to a 256 bit-long fingerprint or "hash", which is usually displayed using 64 hexadecimal characters.
The following shows the SHA256-fingerprint of the content of variable `numbers` from the previous example:
\begin{verbatim}
```{r echo=FALSE, eval=TRUE, results="asis"}
content <- jsonlite::serializeJSON(digest::digest(numbers,algo = "sha256"),pretty=TRUE)
content <- gsub(pattern="\\n",replacement="  \n", x=content)
cat(content)
```
\end{verbatim}
And the following example is a SHA256 fingerprint of the first chapter of Genesis from the Old Testament:
\begin{verbatim}
```{r echo=FALSE, eval=TRUE, results="asis"}
library(logos)
genesis <- select_passage("Genesis",chapter = 1, language = "English", testament = "old")
content <- jsonlite::serializeJSON(digest::digest(genesis,algo = "sha256"),pretty=TRUE)
content <- gsub(pattern="\\n",replacement="  \n", x=content)
cat(content)
```
\end{verbatim}
Note that these exemplary fingerprints have the same length despite both variables having different variable types and sizes.
Due to their small size, fingerprints allow for an efficient comparison of identity of results while avoiding data security and privacy issues as they do not allow the original objects to be reproduced.





In addition to the contents of all newly declared variables in a `reproducibleR`-chunk, the package also stores a fingerprint of the code syntax.
This allows later checks whether a potential non-reproduction is due to a modification of the code chunk since the original computation.
The storage of the fingerprinted information about a code chunk and its computational results is illustrated in the schematic in Figure \@ref(fig:jsonschema).

```{r jsonschema, fig.cap="A JSON file stores fingerprints of all newly declared variables in a reproducibleR-chunk as well as fingerprints of the code chunk syntax. This image by Andreas Brandmaier and Anke Schneider is licensed under [CC BY 4.0]( https://creativecommons.org/licenses/by/4.0/)"}
knitr::include_graphics("img/schema-json-fingerprints_v4.png")
```

## Comparisons of computational results

Once a document is (re-)generated with metadata present (i.e., reproducibleRchunks with matching JSON files), the package will compare all objects that are amenable for reproducibility checks.
Identicality of original and reproduced results is checked with the `all.equal()` function from R's `base` package, which according to the R documentation is a "is a utility to compare R objects x and y testing ‘near equality’. If they are different, comparison is still made to some extent, and a report of the differences is returned." (see `?all.equal` R documentation).

### Types of objects

In principle, any R object is suitable for reproducibility testing no matter how complex.
For creating fingerprints, the package uses R's serialization methods, which take any R object as input.
That is, all of the following variables (`x` of class _integer_, `y` of class _character_, `z` of class _lm_, and `lst` of class _list_) are valid examples for the automated reproducibility checks:

```{r examples, eval=FALSE, echo=TRUE}
  x <- 1:10
  y <- "qr"
  z <- lm(x~1, method=y)
  lst <- list(x, y, z)
```

## Tutorial

In the following, we provide a practical guide on how to use reproducible R code chunks in Markdown documents. First, to install the package, please install the latest version from CRAN^[The latest version can be found in the package repository: https://github.com/brandmaier/reproducibleRchunks]:

```{r eval=FALSE, echo=TRUE}
install.packages("reproducibleRchunks")
```

In your R Markdown file, load the package using  `library(reproducibleRchunks)`, preferably in the first regular R code chunk at the very top of the document.

Now, we define a reproducible code chunk by setting the code chunk language to `reproducibleR`. 
Once this document is rendered the first time, a data file will be created, which stores all reproducible results, which are computed in this code block. 
The name of the file will contain the label name (in this example `helloworld`). 
The following code block contains two reproducible results stored in variables `x` and `y`. 
Those computations can be based on results computed in previous chunks but those variables will not per se be subject to the reproducibility tests.
If the R markdown document is rendered a second time, the computational results are recomputed and compared against the original results. 
For each result, success or failure or reproduction is reported separately.

````markdown
`r ''````{reproducibleR helloworld, echo=TRUE, eval=TRUE}
set.seed(42)
x <- rnorm(10, mean=0, sd=1) 
y <- 4 * 4
```
````

Rendering this markdown twice in a row on the same computer should result in a message of successful replication. 
To break reproducibility, remove or comment out the `set.seed()` command, which sets the random number generator in a reproducible state, and render the document again. 
Now, you should obtain an error message indicating that the result of `x` could not be reproduced while `y` still reproduces as it did not depend on the random number generator.

In practice, we expect that reproducible R chunks will be used both in cases, in which the reproduction report should be displayed (e.g., when developing a data analysis script or reproducing a former one) and those where it should not (e.g., when rendering a presentation or a manuscript for submission to a journal). 
By default, reproduction report statements are produced for each chunk but this default can be changed via the code chunk argument `report=FALSE`.
If users wish to change the default display of reports over the entire document, this can be adjusted via a global knitr option as follows:

```{r echo=TRUE, eval=FALSE}
knitr::opts_chunk$set(report = FALSE)
```

### Changing defaults

Some default behaviors of the package can be changed via R `options()`. The package defines the following options:

`reproducibleRchunks.digits`
: This is the number of digits for rounding of numbers and controls the numeric precision of the reproducibility checks. By default, this is $10$.

`reproducibleRchunks.filetype`
: This is the type of data storage. Currently, we only support the 'json' format; however, this leaves room for other formats to be supported in future. 

`reproducibleRchunks.hashing`
: Boolean. Indicates whether fingerprints should be used (default) or raw values should be stored.

`reproducibleRchunks.hashing_algorithm`
: This is the hashing algorithm, which is used to generate fingerprints of variable contents. By default, the algorithm offered by the `digest` package are used.

`reproducibleRchunks.templates`
: A list with keys corresponding to pandoc output formats and values to templates for formatting the reproducibility reports. See below for more details.

`reproducibleRchunks.prefix`
: This is the prefix for the file containing reproducibility information. The filename will always contain the name of the R Markdown file and the chunk name separated by an underscore. The default prefix is `.repro`.

Here are a few examples how these options can be changed.
First, it is possible to change the precision with which numeric results are stored. By default, this is up to 8 digits. The following line reduces the precision to only four digits after the decimal point:

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.digits = 4)
```

By default, computational results are stored as fingerprints using a hash function. 
To store data in raw format, switch off hashing with the following option:

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.hashing = FALSE)
```

```{r}
# sha benchmark
set.seed(345)
numbers_in_simulation <- 1000
benchmark <- microbenchmark::microbenchmark(digest::sha1(numbers), unit = "seconds", check = "equal", setup = {numbers <- runif(numbers_in_simulation)})

mean_time_s <- summary(benchmark)$mean
```

There are various hashing functions available, which have different features.
They generally differ along three dimensions: speed, chance of collisions, and security.
Generally, hashing functions should map objects of arbitrary size to a fixed-size alphanumeric string.
In this application, the choice of algorithm is not overly crucial.
Supported algorithm (through the digest package, @digest2022) are sha1, crc32, sha256, sha512, xxhash32, xxhash64, murmur32, spookyhash and blake3. 
To reduce chance of collisions, the package defaults to `sha256` which results in fingerprints of 64 hexadecimal characters, which corresponds to a 256-bit fingerprint but comes in cost of some speed (`sha256` of a vector of `r numbers_in_simulation` numbers takes `r mean_time_s` seconds on our standard computers).
We believe that in most cases, the speed factor will be irrelevant but it could become an issue when very large objects (e.g., entire neuroimaging data sets) are fingerprinted.
In that case, users either have to use a faster fingerprinting algorithm or forego fingerprinting raw data and instead fingerprint intermediate or final results only.

```{r eval=FALSE, echo=TRUE}
options(reproducibleRchunks.hashing_algorithm = "sha256")
```

Note that these options can be chosen differently for each chunk.
That is, it is possible, to use fingerprints for storing results of one chunk and plain data storage for results of another chunk.

### Customization

Last, users of this package can customize the appearance of the reproducibility reports. 
This can be done by either using convenient template layouts, which can be tailored to each specific (pandoc) output format, that is, for example a template for HTML file and a different LaTeX template for PDF files. 
Or, users can obtain a summary of the reproduction status of every variable in every chunk. 
This information is returned by function `get_reproducibility_summary()` which returns a `data.frame` with three columns. 
The first column contains the name of the code chunk, the second column contains a variable name, and the third column contains a boolean variable representing whether the reproduction was a success. 
This information can be used to either generate custom reports, such as one general report at the very end of the document, or write reports to a separate file.
The alternative is to just use the default reports that are appended to each code chunk output (as shown in the previous examples). 
Here, users cannot change the content but have some degrees of freedom in styling the output.
The option `reproducibleRchunks.templates` stores the default templates that are used for displaying the report information. 
It is a list of key-value pairs where the key is the final pandoc output format (typically either html, pdf or docx) and the value is a string containing formatting information.
If html output is chosen, it can contain any valid HTML/CSS code, if the output is pdf, it can contain any valid LaTeX code.
These formatting instructions can contain two placeholders, which the package will replace with the title of the report (\${title}) and the content of the report (\${content}).

```{r echo=TRUE, eval=FALSE}
    options(reproducibleRchunks.templates = list(
      html="<div style='border: 3px solid black; 
      padding: 10px 10px 10px 10px; 
      background-color: #EEEEEE;'>
      <h5>${title}</h5>
      ${content}</div>"))
```

For example, the following code reformats the appearance of the code report in PDF documents (via LaTeX), such that the report is enclosed by two horizontal lines (\\hrulefill elements), the title is displayed as a section header, there is a medium skip between title and content, and content is displayed in a small font size:

```{r echo=TRUE, eval=FALSE}
    options(reproducibleRchunks.templates = list(
      latex="\\hrulefill \n \\section{${title}} 
      \\medskip \\small 
      ${content}\n \\hrulefill \n
      ")
    )
```

As mentioned above, if users wish to entirely suppress the default reproducibility reports, they can use the chunk argument `report=FALSE` as part of the code chunk options to suppress the display. 
Note that reproduction is still attempted and reproducibility information is still accessible through the function `get_reproducibility_summary()`.
The other standard chunk options can be used with `reproducibleR`-code chunks as usual. 
In particular, `eval=FALSE` suppressed execution of the code, `echo=FALSE` suppressed that the code is shown and so forth.

### Recommendations

In the following, we give recommendations on how to use the package in typical cases of statistical data analysis and reporting.
A simple way to start is to replace all classic `R` code chunks with `reproducibleR` code chunks.
Then, all variables created in the process of a given data analysis will be tested for reproducibility.
However, we advise against this procedure as it is likely that some variables will not exactly reproduce even though all results that researchers find meaningful will perfectly reproduce.

```{r eval=TRUE, include=FALSE}
library(OpenMx)

data(demoOneFactor)
manifests <- names(demoOneFactor)
latents   <- c("G")

factorModel1 <- mxModel(name="One Factor",
                        type="RAM",
                        manifestVars=manifests,
                        latentVars=latents,
                        mxPath(from=latents, to=manifests),
                        mxPath(from=manifests, arrows=2),
                        mxPath(from=latents, arrows=2, free=FALSE, values=1.0),
                        mxData(observed=cov(demoOneFactor), type="cov", numObs=500)
)

factorFit1 <- mxRun(factorModel1)
summary(factorFit1)
```

Statistical models may often store internal information, which is not necessarily relevant to the statistical result but will strictly lead to a non-reproducibility error as fingerprints of the entire objects will differ.
For example, structural equation models estimated with `OpenMx` [@openmx2] store information about the time elapsed when fitting the model. 
Here is an example of the contents of a simple CFA model from the OpenMx documentation called `factorFit1` run on OpenMx's demonstration dataset `demoOneFactor` including 500 observations on five numeric variables. 
Among information relevant to reproducibility issues, its `output` attribute also contains timing variables `wallTime` and `cpuTime` as well as a timestamp `timestamp`, which will almost always lead to a non-reproducibility error if the entire `factorFit1` object is tested for reproducibility.
For illustration, here is the content of the `cpuTime` variable and the `timestamp` variable for the aforementioned factor model, which was estimated when this manuscript was generated:

```{r eval=TRUE, echo=TRUE}
cat(factorFit1$output$cpuTime)
cat(factorFit1$output$timestamp)
```

Thus, we generally recommend a checkpoint approach, in which reproducibility tests are used only at selected, meaningful stages of the data analysis process and are used only on variables that contain values that are shown and/or interpreted in the scientific report (i.e., effect size point estimates, goodness-of-fit indices, standard error estimates, confidence intervals, test statistics, p values, Bayes factors, etc.).
Specifically, we recommend to use at least the following checkpoints:

- data loading: check whether the loaded raw data are identical to the data loaded in the original analysis
- preprocessing: check whether the preprocessed data (that is, after steps such as outlier removal, aggregation, filtering) are identical to the preprocessed data in the original analysis
- results: check whether the results that are reported in text, tables, and figures are identical to the results of the original analysis.
At the same time, avoid adding automated tests of entire statistical models but focus on the results.

A convenient approach is to use standard wrapper methods that extract relevant numeric quantities from statistical models, such as parameter estimates, effect size estimates, confidence interval limits, p values, fit indices, or Bayes factors. 
To this end, several packages support the generic function `coef()`, which can be used, for example, to extract point estimates from linear regression models.
Other packages offer their own accessor function such as `parTable()` for structural equation models in `lavaan` or `omxGetParameters()`in the aforementioned `OpenMx` model.
The output of the `summary()` function may be a good target for reproducibility checks because it often contains information about parameter estimates and fit statistics; however, in some cases (as with OpenMx models), it may again contain timing information, which will not exactly reproduce.
Last, note that sometimes there may small deviations of exact reproducibility, which can be tolerated.
For example, when comparing reproducibility across machines that work with different numerical precision (32bit vs 64bit), numeric representations may count as reproducible if they are identical up to some numerical precision.
To avoid such problems, `reproducibleRchunks` rounds numeric values to a given precision (by default, ten digits).
Lastly, users may wish to test identity of reproduction up to a certain precision less than the package default.
This could be relevant for algorithms that are fundamentally based on random numbers, such as Monte Carlo methods, bootstrap estimators and similar (even though, their perfect reproducibility should usually be guarantueed by setting a random seed).
In this case, approximate reproducibility tests could be realized by individually adjusting the numeric precision of the tests (see option `reproducibleRchunks.digits` as previously described).
Adherents of the tidyverse approach [@wickham2019welcome] are well advised to use functions `tidy()` and `glance()` from the `broom` [@broom] package.
These functions convert parameter estimates and model fit statistics from various statistical models in R (e.g., anova, glm, coxph, gam, lm, lavaan,smooth.spline, survfit, and others) into a consistent and easily accessible format known as _tibble_. This format is particularly suitable for fingerprinting using our proposed approach.
Here is a brief example of how the `broom` package formats the output of a linear regression model:

```{r echo=TRUE, eval=TRUE}
x <- rnorm(10)
y <- rnorm(10)
broom::tidy(lm(y~x))
```

As a rule of thumb, we recommend to fingerprint the results of all parameter estimates from the `tidy()` function and all model fit statistics from the `glance()` function in a `reproducibleR` chunk. 

# Discussion

We introduced an R package that allows for R code chunks in R Markdown documents that are automatically tested for reproducibility. 
The output of the package is compatible with all pandoc output formats including the standard choices html, pdf, and docx. 
Templates allow for customization of the appearance of reproducibility reports; further, the package allows users to generate entirely customized reproducibility reports either within a given R Markdown document or as separate files.

### Potential long-term issues

In the following, we briefly discuss a few future situations, in which threats to reproducibility or reproducibility checking happened and explain how these situations can be dealt with:

- Original computations were executed and stored in the JSON data file. Later, a reproduction attempt is made using the exact same R Markdown file but on a different computer. This is a classic case of non-reproducibility, which often is due to changes in the software packages and the R version the computations in the R Markdown rely upon [@epskamp2019reproducibility; @peikert2021reproducible]. Note that the goal of this package is not to guarantee reproducibility but to allow for automated testing and reporting of reproducibility. To increase the chances of reproducibility in the first place, various solutions exist [@peikert2021reproducible; @renv2022; @chan2023rang; @nagraj2023pracpac] of which a primary aspect is a way to document and recreate the original computational environment.
- Some computations are executed and their original results are stored in a JSON file as planned. Later on, someone modifies the R code in the R Markdown file, such that the results differ and a failure of reproduction is indicated. This change of code between the original computation and the reproduction attempt is caught by the package because fingerprints of the entire code chunk syntax are stored. Users are informed by a warning in the reproducibility report. The package will still try to reproduce each result and give individual reports on successes and failures.
- The metadata files get lost. Without metadata files, no automated reproducibility check can be made. In this case, it may be a good idea to rerun the analysis in a computing environment as close as possible to the original computing environment and store the newly generated metadata files. It is advisable to manually compare the recreated results to some original reference (e.g., a report or published article) for consistency. 
- The `reproducibleRchunks` package is not available anymore. In this case, all R Markdown code chunks of type `reproducibleR` can be renamed to `R` and the manuscript will at least render even though without automated reproducibility checks. If there is a large number of code chunks, the following line of code can be inserted in the Markdown document in the very first code chunk, which will tell knitr to render all `reproducibleR` chunks as regular R chunks without the need to rename the chunks: `R knitr::knit_engines$set(reproducibleR = knitr::knit_engines$get("R")`.
- `R` is not available anymore. Some day `R` may be superseded by a future programming language and/or there will be no port of `R` or the `reproducibleRchunks` package to a future computing platform. Then, the JSON format will likely still allow the possibility to read out the original computational results in that future programming language as the JSON format itself is open and transparent. 
- A future `R` version changes the way objects are serialized (that is, converted from the internal representation to a byte-stream representation, of which the fingerprint is taken). The metadata contains information about the R version used to generate the metadata, so future versions of our package could easily be adapted to this change. As long as the `digest` package is used for generating fingerprints, the serialization version can be fixed to the version that is used at the time of writing (2024) using this command: `options(serializeVersion=2)`.

### Rigor in software development

In developing this package, we adhere to three major aspects of rigor in scientific software development [@brandmaier2024commentary]. 
First, the package comes with a variety of formal tests (based on the `testthat` package, @testthat2011) to test correct functioning of the package.
Second, we provide documentation in form of this manuscript and an online documentation ([https://github.com/brandmaier/reproducibleRchunks](https://github.com/brandmaier/reproducibleRchunks)). 
Third, bug reports and feature requests can be submitted through our github project website.

### Storing and publishing reproducibility information

We strongly discourage making reproducibility checks on complete raw data. 
This is mainly for two reasons.
First, there may arise unwanted data protection issues when raw data is (accidentally) stored, copied or published via the reproducibility JSON files.
Second, the JSON format is inefficient in terms of storage space required.
It is designed to be human-readable and conformable with versioning tools such as git.
It is not meant for storing large data and will blow up file size by a considerable factor (while at the same time losing numerical precision).
At the same time, ensuring that the data set is identical to the one used in the original analysis was used is essential (see our previous suggestion for checkpoints in a data analysis); that is, raw data in a `data.frame` or similar R structure should be tested for reproducibility but only indirectly using fingerprints.
Note that many fitted statistical models in R also contain raw data, such as (general) linear model fits from the `stats` or `lme4` package or structural equation model fits from the `lavaan` or `OpenMx` packages.
Repeating our previous suggestion, we recommend to only subject parameter estimates and fit indices to the reproducibility checks but not variables containing fitted statistical models.

### Summary

Again, we emphasize that this package is not meant to ensure reproducibility but allows for automatically testing and verifying reproducibility.
For example, it would allow for reproducibility checks in peer review as suggested by @cruewell2023 could be automated using this package.
According to the authors' convention, the current approach relies on testing whether results are _exactly reproducible_, which is consistent with the idea of awarding an Open Data badge.
If there was a demand, the package could be extended to support the more fine-grained judgements suggested by @cruewell2023, such as _essentially reproducible_ (minor deviations in the decimals), _partially reproducible_ (minor deviations but the results were mostly numerically consistent),or _mostly not reproducible_ (major deviations).
Note, however, that the consistent use of R Markdown already eliminates some sources of irreproducibility, such as copy-and-paste errors [@peikert2021reproducible].
Even though, a variety of approaches have been suggested to ensure reproducibility of computations in R Markdown documents [@peikert2021reproducible; @renv2022; @chan2023rang; @nagraj2023pracpac], we ourselves have encountered various situations, in which such approaches (including our own) failed. 
First and foremost, many approaches rely on further software packages such as Docker, which essentially provide virtual environments that execute code under identical conditions on different machines (or the same machine at different time points, e.g., before and after upgrade of R or some or all packages used).
On some machines, Docker may simply be not available, either because it is not (yet) available on a certain newly introduced hardware (e.g., this happened when Apple switched to their own processor brand) or because a user does not have admin privileges for installing Docker in the first place.
Further, some approaches rely on service providers such as Microsoft's MRAN archive, which a while ago was unexpectedly terminated. 
These different weak points now and then lead to situations, in which users will try to locally reproduce historic computational results and they may want to ensure that these reproductions were succesful.
With our package, the success of such reproduction attempts becomes easily and formally testable.

We hope the suggested approach helps raising awareness for the importance of reproduciblity and increasing the visibility of potential non-reproducibility issues in R code, eventually increasing quality and credibility of digital research assets.
\newpage

# References

::: {#refs custom-style="Bibliography"}
:::

# Contributions

The authors made the following contributions. AB: Conceptualization, Methodology, Software, Writing - Original Draft Preparation, Writing - Review & Editing; AP: Writing - Review & Editing, Methodology.

# Acknowledgements

We thank Leonie Hagitte for providing comments on an earlier version of the manuscript.

# Competing Interests

The authors declare no competing interests.

# Data accessibility statement 

The R package described in this article can be downloaded from https://github.com/brandmaier/reproducibleRchunks. The manuscript source code is available at https://github.com/brandmaier/reproducibleRchunks-manuscript.

# Figure titles and legends

```{r legends, results="asis"}
for (i in 1:length(legends)) {
  cat("Figure ",i ,".", legends[[i]],"\n\n")
}

```
